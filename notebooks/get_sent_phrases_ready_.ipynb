{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"240420_sentences_each2500plus_with_ptb.csv\")\n",
    "#d = pd.read_csv(\"240420_remaining_DATASETo71_with_ptb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_correct</th>\n",
       "      <th>score_g</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Under Canberra's immigration policy, asylum-se...</td>\n",
       "      <td>Canberra 's immigration policy|Canberra 's|asy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Baritone roles are usually villainous charact...</td>\n",
       "      <td>Baritone roles|villainous characters , such as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Five large high-volume, low-speed fans will co...</td>\n",
       "      <td>Five large high-volume , low-speed fans|diners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Market players say it could take months for lo...</td>\n",
       "      <td>Market players|it|months for long-term investo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>But if we had to keep relying on our past rese...</td>\n",
       "      <td>we|our past reserves|I|we|them|had to keep rel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  score_correct  score_g sentence_id  \\\n",
       "0    NaN            2.0      2.0           0   \n",
       "1    NaN            1.0      1.0           1   \n",
       "2    NaN            3.0      3.0           2   \n",
       "3    NaN            1.0      1.0           3   \n",
       "4    NaN            3.0      3.0           4   \n",
       "\n",
       "                                                text  \\\n",
       "0  Under Canberra's immigration policy, asylum-se...   \n",
       "1  \"Baritone roles are usually villainous charact...   \n",
       "2  Five large high-volume, low-speed fans will co...   \n",
       "3  Market players say it could take months for lo...   \n",
       "4  But if we had to keep relying on our past rese...   \n",
       "\n",
       "                                             phrases  \n",
       "0  Canberra 's immigration policy|Canberra 's|asy...  \n",
       "1  Baritone roles|villainous characters , such as...  \n",
       "2  Five large high-volume , low-speed fans|diners...  \n",
       "3  Market players|it|months for long-term investo...  \n",
       "4  we|our past reserves|I|we|them|had to keep rel...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(d.index))\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13172\n"
     ]
    }
   ],
   "source": [
    "#d = d[d['phrases'].notna()]\n",
    "print(len(d.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-1251897029d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sentence_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6209\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6210\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6211\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6213\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5416\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5417\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[0;32m-> 5418\u001b[0;31m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[1;32m   5419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m             b = make_block(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[0;34m(self, to_concat, placement)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[1;32m    367\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[0;32m--> 368\u001b[0;31m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[1;32m    369\u001b[0m         return self.make_block_same_class(\n\u001b[1;32m    370\u001b[0m             values, placement=placement or slice(0, len(values), 1))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "d = d.replace(np.nan, None, regex=True)\n",
    "\n",
    "for i in d.index:\n",
    "    dic = {}\n",
    "    dic.update({'sentence_id':d.loc[i,'sentence_id']})\n",
    "    dic.update({'text':d.loc[i,'text']})\n",
    "    data = data.append(dic,ignore_index=True)\n",
    "    \n",
    "    if d.loc[i,'phrases'] is not None:\n",
    "        for x in d.loc[i,'phrases'].split('|'):\n",
    "            if x != '':\n",
    "                dic = {}\n",
    "                dic.update({'sentence_id':d.loc[i,'sentence_id']})\n",
    "                dic.update({'text':x})\n",
    "                data = data.append(dic,ignore_index=True)\n",
    "                \n",
    "    for x in d.loc[i,'text'].split():\n",
    "        if x != '':\n",
    "            dic = {}\n",
    "            dic.update({'sentence_id':d.loc[i,'sentence_id']})\n",
    "            dic.update({'text':x})\n",
    "            data = data.append(dic,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countOfWords(s):\n",
    "    lst = s.split(\" \")\n",
    "    if all(char.isdigit() for char in lst): return None\n",
    "    else: return len(lst)\n",
    "    \n",
    "print(data.head())\n",
    "data['#words'] = data['text'].apply(lambda x: get_countOfWords(x))\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['phrases'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>#words</th>\n",
       "      <th>phrase_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>But if we had to keep relying on our past rese...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0-p1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>we</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>our past reserves</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0-p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>a while</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0-p5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>we</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>them</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>had to keep relying on our past reserves</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0-p8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>to keep relying on our past reserves</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0-p9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>keep relying on our past reserves</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0-p10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>relying on our past reserves</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0-p11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>think after a while we would have exhausted them</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0-p12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.0</td>\n",
       "      <td>would have exhausted them</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0-p13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.0</td>\n",
       "      <td>have exhausted them</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0-p14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.0</td>\n",
       "      <td>exhausted them</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0-p15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>But</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>if</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>we</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>had</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>to</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>keep</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.0</td>\n",
       "      <td>relying</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>on</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.0</td>\n",
       "      <td>our</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.0</td>\n",
       "      <td>past</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.0</td>\n",
       "      <td>reserves,</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>think</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.0</td>\n",
       "      <td>after</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.0</td>\n",
       "      <td>a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0-p30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>272.0</td>\n",
       "      <td>support</td>\n",
       "      <td>1.0</td>\n",
       "      <td>272.0-p21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>272.0</td>\n",
       "      <td>us,</td>\n",
       "      <td>1.0</td>\n",
       "      <td>272.0-p22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>272.0</td>\n",
       "      <td>said</td>\n",
       "      <td>1.0</td>\n",
       "      <td>272.0-p23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>272.0</td>\n",
       "      <td>Wilfred.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>272.0-p24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>310.0</td>\n",
       "      <td>Measures to enhance security in Little India.</td>\n",
       "      <td>7.0</td>\n",
       "      <td>310.0-p1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>310.0</td>\n",
       "      <td>Measures</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>310.0</td>\n",
       "      <td>security</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>310.0</td>\n",
       "      <td>Little India</td>\n",
       "      <td>2.0</td>\n",
       "      <td>310.0-p4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>310.0</td>\n",
       "      <td>to enhance security in Little India</td>\n",
       "      <td>6.0</td>\n",
       "      <td>310.0-p5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>310.0</td>\n",
       "      <td>enhance security in Little India</td>\n",
       "      <td>5.0</td>\n",
       "      <td>310.0-p6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>310.0</td>\n",
       "      <td>Measures</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>310.0</td>\n",
       "      <td>to</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>310.0</td>\n",
       "      <td>enhance</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>310.0</td>\n",
       "      <td>security</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>310.0</td>\n",
       "      <td>in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>310.0</td>\n",
       "      <td>Little</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>310.0</td>\n",
       "      <td>India.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0-p13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>317.0</td>\n",
       "      <td>Our traditional markets the heartland of the ...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>317.0-p1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>317.0</td>\n",
       "      <td>Our traditional markets</td>\n",
       "      <td>3.0</td>\n",
       "      <td>317.0-p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>317.0</td>\n",
       "      <td>the heartland of the usiness</td>\n",
       "      <td>5.0</td>\n",
       "      <td>317.0-p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>317.0</td>\n",
       "      <td>the heartland</td>\n",
       "      <td>2.0</td>\n",
       "      <td>317.0-p4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>317.0</td>\n",
       "      <td>the usiness</td>\n",
       "      <td>2.0</td>\n",
       "      <td>317.0-p5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>317.0</td>\n",
       "      <td>Australia , Japan , United States and Europe</td>\n",
       "      <td>8.0</td>\n",
       "      <td>317.0-p6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>317.0</td>\n",
       "      <td>she</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317.0-p7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>317.0</td>\n",
       "      <td>is really in Australia , Japan , United States...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>317.0-p8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>317.0</td>\n",
       "      <td>said</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317.0-p9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>317.0</td>\n",
       "      <td>Our</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317.0-p10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>317.0</td>\n",
       "      <td>traditional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317.0-p11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>317.0</td>\n",
       "      <td>markets</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317.0-p12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>317.0</td>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317.0-p13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_id                                               text  #words  \\\n",
       "0             4.0  But if we had to keep relying on our past rese...    21.0   \n",
       "1             4.0                                                 we     1.0   \n",
       "2             4.0                                  our past reserves     3.0   \n",
       "3             4.0                                                  I     1.0   \n",
       "4             4.0                                            a while     2.0   \n",
       "5             4.0                                                 we     1.0   \n",
       "6             4.0                                               them     1.0   \n",
       "7             4.0           had to keep relying on our past reserves     8.0   \n",
       "8             4.0               to keep relying on our past reserves     7.0   \n",
       "9             4.0                  keep relying on our past reserves     6.0   \n",
       "10            4.0                       relying on our past reserves     5.0   \n",
       "11            4.0   think after a while we would have exhausted them     9.0   \n",
       "12            4.0                          would have exhausted them     4.0   \n",
       "13            4.0                                have exhausted them     3.0   \n",
       "14            4.0                                     exhausted them     2.0   \n",
       "15            4.0                                                But     1.0   \n",
       "16            4.0                                                 if     1.0   \n",
       "17            4.0                                                 we     1.0   \n",
       "18            4.0                                                had     1.0   \n",
       "19            4.0                                                 to     1.0   \n",
       "20            4.0                                               keep     1.0   \n",
       "21            4.0                                            relying     1.0   \n",
       "22            4.0                                                 on     1.0   \n",
       "23            4.0                                                our     1.0   \n",
       "24            4.0                                               past     1.0   \n",
       "25            4.0                                          reserves,     1.0   \n",
       "26            4.0                                                  I     1.0   \n",
       "27            4.0                                              think     1.0   \n",
       "28            4.0                                              after     1.0   \n",
       "29            4.0                                                  a     1.0   \n",
       "...           ...                                                ...     ...   \n",
       "978         272.0                                            support     1.0   \n",
       "979         272.0                                                us,     1.0   \n",
       "980         272.0                                               said     1.0   \n",
       "981         272.0                                           Wilfred.     1.0   \n",
       "982         310.0      Measures to enhance security in Little India.     7.0   \n",
       "983         310.0                                           Measures     1.0   \n",
       "984         310.0                                           security     1.0   \n",
       "985         310.0                                       Little India     2.0   \n",
       "986         310.0                to enhance security in Little India     6.0   \n",
       "987         310.0                   enhance security in Little India     5.0   \n",
       "988         310.0                                           Measures     1.0   \n",
       "989         310.0                                                 to     1.0   \n",
       "990         310.0                                            enhance     1.0   \n",
       "991         310.0                                           security     1.0   \n",
       "992         310.0                                                 in     1.0   \n",
       "993         310.0                                             Little     1.0   \n",
       "994         310.0                                             India.     1.0   \n",
       "995         317.0   Our traditional markets the heartland of the ...    20.0   \n",
       "996         317.0                            Our traditional markets     3.0   \n",
       "997         317.0                       the heartland of the usiness     5.0   \n",
       "998         317.0                                      the heartland     2.0   \n",
       "999         317.0                                        the usiness     2.0   \n",
       "1000        317.0       Australia , Japan , United States and Europe     8.0   \n",
       "1001        317.0                                                she     1.0   \n",
       "1002        317.0  is really in Australia , Japan , United States...    11.0   \n",
       "1003        317.0                                               said     1.0   \n",
       "1004        317.0                                                Our     1.0   \n",
       "1005        317.0                                        traditional     1.0   \n",
       "1006        317.0                                            markets     1.0   \n",
       "1007        317.0                                                the     1.0   \n",
       "\n",
       "      phrase_id  \n",
       "0        4.0-p1  \n",
       "1        4.0-p2  \n",
       "2        4.0-p3  \n",
       "3        4.0-p4  \n",
       "4        4.0-p5  \n",
       "5        4.0-p6  \n",
       "6        4.0-p7  \n",
       "7        4.0-p8  \n",
       "8        4.0-p9  \n",
       "9       4.0-p10  \n",
       "10      4.0-p11  \n",
       "11      4.0-p12  \n",
       "12      4.0-p13  \n",
       "13      4.0-p14  \n",
       "14      4.0-p15  \n",
       "15      4.0-p16  \n",
       "16      4.0-p17  \n",
       "17      4.0-p18  \n",
       "18      4.0-p19  \n",
       "19      4.0-p20  \n",
       "20      4.0-p21  \n",
       "21      4.0-p22  \n",
       "22      4.0-p23  \n",
       "23      4.0-p24  \n",
       "24      4.0-p25  \n",
       "25      4.0-p26  \n",
       "26      4.0-p27  \n",
       "27      4.0-p28  \n",
       "28      4.0-p29  \n",
       "29      4.0-p30  \n",
       "...         ...  \n",
       "978   272.0-p21  \n",
       "979   272.0-p22  \n",
       "980   272.0-p23  \n",
       "981   272.0-p24  \n",
       "982    310.0-p1  \n",
       "983    310.0-p2  \n",
       "984    310.0-p3  \n",
       "985    310.0-p4  \n",
       "986    310.0-p5  \n",
       "987    310.0-p6  \n",
       "988    310.0-p7  \n",
       "989    310.0-p8  \n",
       "990    310.0-p9  \n",
       "991   310.0-p10  \n",
       "992   310.0-p11  \n",
       "993   310.0-p12  \n",
       "994   310.0-p13  \n",
       "995    317.0-p1  \n",
       "996    317.0-p2  \n",
       "997    317.0-p3  \n",
       "998    317.0-p4  \n",
       "999    317.0-p5  \n",
       "1000   317.0-p6  \n",
       "1001   317.0-p7  \n",
       "1002   317.0-p8  \n",
       "1003   317.0-p9  \n",
       "1004  317.0-p10  \n",
       "1005  317.0-p11  \n",
       "1006  317.0-p12  \n",
       "1007  317.0-p13  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## adding phrase id \n",
    "dfg = data.groupby(['sentence_id'])\n",
    "for id, grp in dfg:\n",
    "    sid = str(id) + '-p'\n",
    "    k=1\n",
    "    for i in grp.index:\n",
    "        data.loc[i,'phrase_id'] = sid + str(k)\n",
    "        k += 1\n",
    "        \n",
    "data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = pd.merge(data, d[['sentence_id','score_correct']], on='sentence_id')\n",
    "ndata.to_csv(\"sentences_phrases_220420.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>#words</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4727</td>\n",
       "      <td>Unlike stray cats and dogs, stray salamanders ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4727-p1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4727</td>\n",
       "      <td>stray cats and dogs</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4727-p2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4727</td>\n",
       "      <td>stray salamanders</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4727-p3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4727</td>\n",
       "      <td>are more uncommon</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4727-p4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4727</td>\n",
       "      <td>Unlike</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4727-p5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                               text  #words  \\\n",
       "0        4727  Unlike stray cats and dogs, stray salamanders ...    10.0   \n",
       "1        4727                                stray cats and dogs     4.0   \n",
       "2        4727                                  stray salamanders     2.0   \n",
       "3        4727                                  are more uncommon     3.0   \n",
       "4        4727                                             Unlike     1.0   \n",
       "\n",
       "  phrase_id  score_correct  \n",
       "0   4727-p1            2.0  \n",
       "1   4727-p2            2.0  \n",
       "2   4727-p3            2.0  \n",
       "3   4727-p4            2.0  \n",
       "4   4727-p5            2.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing the extra sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\tsentence_id\ttext\n",
    "sentiment_score\t\t\n",
    "0.0\t26\t26\n",
    "1.0\t117\t117\n",
    "2.0\t483\t483\n",
    "3.0\t586\t586\n",
    "4.0\t21\t21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-3a7fe296f7b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'-s'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mndata_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndata_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndata_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6209\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6210\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6211\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6213\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m             b = make_block(\n\u001b[0;32m-> 5421\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5422\u001b[0m                 placement=placement)\n\u001b[1;32m   5423\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5577\u001b[0m                 \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5579\u001b[0;31m         \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5581\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36m_concat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ndata_ = pd.DataFrame()\n",
    "indices = [x for x in ndata.index if '-s' in ndata.loc[x,'sentence_id']]\n",
    "for i in indices:\n",
    "    ndata_ = ndata_.append(ndata.loc[i,],ignore_index=True)\n",
    "len(ndata_.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out the neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "usual_scores = ndata[ndata['#words'] < 4]\n",
    "\n",
    "usual_scores.to_csv(\"2orlesswords_get_scores.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427606"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ndata['score_correct'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74169"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk = ndata[ndata['#words']>3]\n",
    "print(len(mturk.index))\n",
    "# mturk_final = mturk[mturk['score_correct'].isnull()]\n",
    "\n",
    "mturk_ = pd.DataFrame()\n",
    "indices = [x for x in mturk.index if not mturk.loc[x,'phrase_id'].endswith('-p1')]\n",
    "for i in indices:\n",
    "    mturk_=mturk_.append(mturk.loc[i,],ignore_index=True)\n",
    "len(mturk_.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#words</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score_correct</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4727-p2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4727</td>\n",
       "      <td>stray cats and dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1857-p4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>the aftermath of a Boko Haram attack on a village</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1857-p6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack on a village</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1857-p7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1857-p11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>shows the aftermath of a Boko Haram attack on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #words phrase_id  score_correct sentence_id  \\\n",
       "0     4.0   4727-p2            2.0        4727   \n",
       "1    10.0   1857-p4            1.0        1857   \n",
       "2     7.0   1857-p6            1.0        1857   \n",
       "3     4.0   1857-p7            1.0        1857   \n",
       "4    20.0  1857-p11            1.0        1857   \n",
       "\n",
       "                                                text  \n",
       "0                                stray cats and dogs  \n",
       "1  the aftermath of a Boko Haram attack on a village  \n",
       "2                   a Boko Haram attack on a village  \n",
       "3                                a Boko Haram attack  \n",
       "4  shows the aftermath of a Boko Haram attack on ...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_.to_csv(\"mturk_230420_sp.csv\")\n",
    "mturk_['text'].to_csv(\"input_mturk_230420.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_posTags(s):\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "mturk_['pos_tags'] = mturk_['text'].apply(lambda x : get_posTags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#words</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score_correct</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4727-p2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4727</td>\n",
       "      <td>stray cats and dogs</td>\n",
       "      <td>[(stray, NN), (cats, NNS), (and, CC), (dogs, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1857-p4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>the aftermath of a Boko Haram attack on a village</td>\n",
       "      <td>[(the, DT), (aftermath, NN), (of, IN), (a, DT)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1857-p6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack on a village</td>\n",
       "      <td>[(a, DT), (Boko, NNP), (Haram, NNP), (attack, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1857-p7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack</td>\n",
       "      <td>[(a, DT), (Boko, NNP), (Haram, NNP), (attack, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1857-p11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>shows the aftermath of a Boko Haram attack on ...</td>\n",
       "      <td>[(shows, VBZ), (the, DT), (aftermath, NN), (of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #words phrase_id  score_correct sentence_id  \\\n",
       "0     4.0   4727-p2            2.0        4727   \n",
       "1    10.0   1857-p4            1.0        1857   \n",
       "2     7.0   1857-p6            1.0        1857   \n",
       "3     4.0   1857-p7            1.0        1857   \n",
       "4    20.0  1857-p11            1.0        1857   \n",
       "\n",
       "                                                text  \\\n",
       "0                                stray cats and dogs   \n",
       "1  the aftermath of a Boko Haram attack on a village   \n",
       "2                   a Boko Haram attack on a village   \n",
       "3                                a Boko Haram attack   \n",
       "4  shows the aftermath of a Boko Haram attack on ...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(stray, NN), (cats, NNS), (and, CC), (dogs, N...  \n",
       "1  [(the, DT), (aftermath, NN), (of, IN), (a, DT)...  \n",
       "2  [(a, DT), (Boko, NNP), (Haram, NNP), (attack, ...  \n",
       "3  [(a, DT), (Boko, NNP), (Haram, NNP), (attack, ...  \n",
       "4  [(shows, VBZ), (the, DT), (aftermath, NN), (of...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_.to_csv(\"230420_MTurk_input_with_POSTags.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74169"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mturk_.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72892"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mturk_.drop_duplicates(['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def group_duplicate_index(df):\n",
    "    a = df.values\n",
    "    sidx = np.lexsort(a.T)\n",
    "    b = a[sidx]\n",
    "\n",
    "    m = np.concatenate(([False], (b[1:] == b[:-1]).all(1), [False] ))\n",
    "    idx = np.flatnonzero(m[1:] != m[:-1])\n",
    "    I = df.index[sidx].tolist()       \n",
    "    return [I[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]\n",
    "\n",
    "\n",
    "dlist = group_duplicate_index(mturk_[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grp in dlist:\n",
    "    count=0\n",
    "    for i in grp:\n",
    "        if count > 1:\n",
    "            mturk_.loc[i,'duplicate'] = 1\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#words</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score_correct</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4727-p2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4727</td>\n",
       "      <td>stray cats and dogs</td>\n",
       "      <td>[('stray', 'NN'), ('cats', 'NNS'), ('and', 'CC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1857-p4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>the aftermath of a Boko Haram attack on a village</td>\n",
       "      <td>[('the', 'DT'), ('aftermath', 'NN'), ('of', 'I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1857-p6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack on a village</td>\n",
       "      <td>[('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1857-p7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack</td>\n",
       "      <td>[('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1857-p11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>shows the aftermath of a Boko Haram attack on ...</td>\n",
       "      <td>[('shows', 'VBZ'), ('the', 'DT'), ('aftermath'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  #words phrase_id  score_correct sentence_id  \\\n",
       "0           0     4.0   4727-p2            2.0        4727   \n",
       "1           1    10.0   1857-p4            1.0        1857   \n",
       "2           2     7.0   1857-p6            1.0        1857   \n",
       "3           3     4.0   1857-p7            1.0        1857   \n",
       "4           4    20.0  1857-p11            1.0        1857   \n",
       "\n",
       "                                                text  \\\n",
       "0                                stray cats and dogs   \n",
       "1  the aftermath of a Boko Haram attack on a village   \n",
       "2                   a Boko Haram attack on a village   \n",
       "3                                a Boko Haram attack   \n",
       "4  shows the aftermath of a Boko Haram attack on ...   \n",
       "\n",
       "                                            pos_tags  duplicate score  \n",
       "0  [('stray', 'NN'), ('cats', 'NNS'), ('and', 'CC...        NaN  None  \n",
       "1  [('the', 'DT'), ('aftermath', 'NN'), ('of', 'I...        NaN  None  \n",
       "2  [('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...        NaN  None  \n",
       "3  [('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...        NaN  None  \n",
       "4  [('shows', 'VBZ'), ('the', 'DT'), ('aftermath'...        NaN  None  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tagslist1 = ['NNP','DT','CD','FW','CC']\n",
    "tagslist2 = ['NNP','NNS']\n",
    "tagslist3 = ['NNP']\n",
    "\n",
    "def is_neutral(taglist, tagslist):\n",
    "    tags =[]\n",
    "    lst = taglist.split(',')\n",
    "    lst = lst[1::2]\n",
    "    for i in lst:\n",
    "        tag = re.sub(\"[^A-Z]+\",'', i)\n",
    "        tags.append(tag)\n",
    "    tags = [x for x in tags if x!= '']\n",
    "    if all(elem in tagslist for elem in tags):\n",
    "        return 2\n",
    "    \n",
    "mturk_['score_1'] = mturk_['pos_tags'].apply(lambda x: is_neutral(x, tagslist1))\n",
    "mturk_['score_2'] = mturk_['pos_tags'].apply(lambda x: is_neutral(x, tagslist2))\n",
    "mturk_['score_3'] = mturk_['pos_tags'].apply(lambda x: is_neutral(x, tagslist3))\n",
    "#mturk_['score_4'] = mturk_['pos_tags'].apply(lambda x: is_neutral(x, tagslist4))\n",
    "mturk_['score'] = mturk_[['score_1','score_2','score_3']].apply(lambda x: ''.join(x.dropna().astype(str)),axis=1)\n",
    "## change the 'score' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_n = mturk_[mturk_['score'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#words</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score_correct</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>score</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>score_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4727-p2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4727</td>\n",
       "      <td>stray cats and dogs</td>\n",
       "      <td>[('stray', 'NN'), ('cats', 'NNS'), ('and', 'CC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1857-p4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>the aftermath of a Boko Haram attack on a village</td>\n",
       "      <td>[('the', 'DT'), ('aftermath', 'NN'), ('of', 'I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1857-p6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack on a village</td>\n",
       "      <td>[('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1857-p7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>a Boko Haram attack</td>\n",
       "      <td>[('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1857-p11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1857</td>\n",
       "      <td>shows the aftermath of a Boko Haram attack on ...</td>\n",
       "      <td>[('shows', 'VBZ'), ('the', 'DT'), ('aftermath'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  #words phrase_id  score_correct sentence_id  \\\n",
       "0           0     4.0   4727-p2            2.0        4727   \n",
       "1           1    10.0   1857-p4            1.0        1857   \n",
       "2           2     7.0   1857-p6            1.0        1857   \n",
       "3           3     4.0   1857-p7            1.0        1857   \n",
       "4           4    20.0  1857-p11            1.0        1857   \n",
       "\n",
       "                                                text  \\\n",
       "0                                stray cats and dogs   \n",
       "1  the aftermath of a Boko Haram attack on a village   \n",
       "2                   a Boko Haram attack on a village   \n",
       "3                                a Boko Haram attack   \n",
       "4  shows the aftermath of a Boko Haram attack on ...   \n",
       "\n",
       "                                            pos_tags  duplicate score  \\\n",
       "0  [('stray', 'NN'), ('cats', 'NNS'), ('and', 'CC...        NaN         \n",
       "1  [('the', 'DT'), ('aftermath', 'NN'), ('of', 'I...        NaN         \n",
       "2  [('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...        NaN         \n",
       "3  [('a', 'DT'), ('Boko', 'NNP'), ('Haram', 'NNP'...        NaN         \n",
       "4  [('shows', 'VBZ'), ('the', 'DT'), ('aftermath'...        NaN         \n",
       "\n",
       "   score_1  score_2  score_3  score_4  \n",
       "0      NaN      NaN      NaN      NaN  \n",
       "1      NaN      NaN      NaN      NaN  \n",
       "2      NaN      NaN      NaN      NaN  \n",
       "3      NaN      NaN      NaN      NaN  \n",
       "4      NaN      NaN      NaN      NaN  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1823"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mturk_n.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NAS/home01/toshal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "del mturk_n['score_1']\n",
    "del mturk_n['score_2']\n",
    "del mturk_n['score_3']\n",
    "# del mturk_n['score_4']\n",
    "mturk_n['score'] = 2\n",
    "mturk_n.to_csv(\"240420_selected_nutral_phrases.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
